{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be sourced from [Brisbane City Council Library Checkouts](https://www.data.brisbane.qld.gov.au/data/dataset/library-checkouts-branch-date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set up the session\n",
    "2. Set up the context\n",
    "3. Raise the log level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the SparkSession and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 17:34:57 WARN Utils: Your hostname, spyro-IdeaPad-5-15ITL05 resolves to a loopback address: 127.0.1.1; using 192.168.43.211 instead (on interface wlp0s20f3)\n",
      "22/10/06 17:34:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/06 17:34:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .appName(\"Demo\")\n",
    "            .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in the csv\n",
    "2. Print the record count\n",
    "3. Print the schema\n",
    "4. Print 3 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 59593\n",
      "\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- call_number: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_type_code: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- checkout_library: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------+-----------+--------------+--------------+--------+----------------+--------------+\n",
      "|               title|       author|call_number|       item_id|item_type_code|     age|checkout_library|          date|\n",
      "+--------------------+-------------+-----------+--------------+--------------+--------+----------------+--------------+\n",
      "|\"Are we nearly th...|         null|821.008 ARE|34000061508842|    NONFICTION|JUVENILE|             TWG|20220607131747|\n",
      "| \"D\" is for deadbeat|Grafton, Sue,|    CD-BOOK|34000099825432|       CD-BOOK|   ADULT|             ASH|20220607105013|\n",
      "| \"F\" is for fugitive|Grafton, Sue,|    CD-BOOK|34000099825382|       CD-BOOK|   ADULT|             ASH|20220607105014|\n",
      "+--------------------+-------------+-----------+--------------+--------------+--------+----------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_in = spark.read.csv(\"library_checkouts_202206.csv\", header=True, quote=\"\\\"\", escape=\"\\\"\").drop(\"status\", \"language\")\n",
    "\n",
    "print(f\"Number of records: {df_in.count()}\\n\")\n",
    "\n",
    "df_in.printSchema()\n",
    "\n",
    "df_in.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in a mapping for library_code -> library_name\n",
    "2. Cast the date column to timestamp\n",
    "3. Apply the library_code mapping\n",
    "4. Check the date column was correctly remapped\n",
    "4. Print 3 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField('date', TimestampType(), True) \n",
      "\n",
      "+------------+--------------------+-------------+-----------+--------------+--------------+--------+-------------------+------------+\n",
      "|library_code|               title|       author|call_number|       item_id|item_type_code|     age|               date|library_name|\n",
      "+------------+--------------------+-------------+-----------+--------------+--------------+--------+-------------------+------------+\n",
      "|         TWG|\"Are we nearly th...|         null|821.008 ARE|34000061508842|    NONFICTION|JUVENILE|2022-06-07 13:17:47|     Toowong|\n",
      "|         ASH| \"D\" is for deadbeat|Grafton, Sue,|    CD-BOOK|34000099825432|       CD-BOOK|   ADULT|2022-06-07 10:50:13|    Ashgrove|\n",
      "|         ASH| \"F\" is for fugitive|Grafton, Sue,|    CD-BOOK|34000099825382|       CD-BOOK|   ADULT|2022-06-07 10:50:14|    Ashgrove|\n",
      "+------------+--------------------+-------------+-----------+--------------+--------------+--------+-------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library_map = spark.read.csv(\"library_mapping.csv\", header=True)\n",
    "\n",
    "df = (df_in.withColumn(\"date\", F.to_timestamp(F.col(\"date\"), \"yyyyMMddHHmmss\"))\n",
    "        .withColumnRenamed(\"checkout_library\", \"library_code\")\n",
    "        .join(library_map, \"library_code\", \"left\"))\n",
    "\n",
    "print(df.schema[\"date\"], \"\\n\")\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Print the number of libraries\n",
    "2. Print the top 3 libraries by number of checkouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct libraries: 36\n",
      "\n",
      "Top 3 libraries by number of checkouts\n",
      "+---------------+-----+\n",
      "|   library_name|count|\n",
      "+---------------+-----+\n",
      "|      Chermside| 4292|\n",
      "|Sunnybank Hills| 3747|\n",
      "|       Ashgrove| 3592|\n",
      "+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library_count = (df.select(\"library_name\")\n",
    "                   .distinct()\n",
    "                   .count())\n",
    "\n",
    "print(f\"Distinct libraries: {library_count}\\n\")\n",
    "\n",
    "print(\"Top 3 libraries by number of checkouts\")\n",
    "\n",
    "(df.groupBy(\"library_name\")\n",
    "   .count()\n",
    "   .orderBy(\"count\", ascending=False)\n",
    "   .show(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which titles are the most popular?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Print the top 10 titles by number of checkouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------+-----+\n",
      "|title                         |item_type_code|count|\n",
      "+------------------------------+--------------+-----+\n",
      "|The New idea.                 |AD-MAGS       |135  |\n",
      "|Woman's day.                  |AD-MAGS       |133  |\n",
      "|The Australian women's weekly.|AD-MAGS       |118  |\n",
      "|Who weekly.                   |AD-MAGS       |116  |\n",
      "|Hello                         |AD-MAGS       |114  |\n",
      "|New scientist. (2021 onwards) |AD-MAGS       |111  |\n",
      "|Australian house and garden.  |AD-MAGS       |105  |\n",
      "|FASTBACK - The bad guys       |FBKIDS        |98   |\n",
      "|Country style                 |AD-MAGS       |84   |\n",
      "|Australian home beautiful.    |AD-MAGS       |83   |\n",
      "+------------------------------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy(\"title\", \"item_type_code\")\n",
    "   .count()\n",
    "   .orderBy(\"count\", ascending=False)\n",
    "   .show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the most popular title in each item type?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the checkout count by title and category\n",
    "2. Rank the titles within each category type\n",
    "3. Select the top ranked title (in each category)\n",
    "4. Print the top 10 top ranked titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+--------------+-----+\n",
      "|title                                                    |item_type_code|count|\n",
      "+---------------------------------------------------------+--------------+-----+\n",
      "|The New idea.                                            |AD-MAGS       |135  |\n",
      "|FASTBACK - The bad guys                                  |FBKIDS        |98   |\n",
      "|The bad guys.                                            |JU-PBK        |71   |\n",
      "|Bluey.                                                   |DVD           |49   |\n",
      "|One piece.                                               |GRAPHICNOV    |39   |\n",
      "|Pok�mon Magazine                                         |JU-MAGS       |36   |\n",
      "|Sparring partners /                                      |AD-PBK        |31   |\n",
      "|Independent and unofficial guide to your minecraft world.|YA-MAGS       |29   |\n",
      "|Ming pao weekly                                          |LOTE-MAG      |21   |\n",
      "|FASTBACK - Sparring partners                             |FASTBACK      |19   |\n",
      "+---------------------------------------------------------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy(\"title\", \"item_type_code\")\n",
    "   .count()\n",
    "   .withColumn(\"rank\", F.rank().over(Window.partitionBy(\"item_type_code\").orderBy(F.desc(\"count\"))))\n",
    "   .filter(F.col(\"rank\") == 1)\n",
    "   .drop(\"rank\")\n",
    "   .orderBy(\"count\", ascending=False)\n",
    "   .show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the borrowing pattern look like for a specific library?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Print 20 consecutive checkouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+--------------+--------+-------------------+\n",
      "|library_code|library_name|author            |item_type_code|age     |date               |\n",
      "+------------+------------+------------------+--------------+--------+-------------------+\n",
      "|ANN         |Annerley    |Fontana, Shea,    |JU-PBK        |JUVENILE|2022-06-07 06:22:16|\n",
      "|ANN         |Annerley    |Doyle, Glennon,   |BIOGRAPHY     |ADULT   |2022-06-07 06:22:23|\n",
      "|ANN         |Annerley    |Marchetta, Melina,|JU-PBK        |JUVENILE|2022-06-07 06:23:36|\n",
      "|ANN         |Annerley    |Loreau, Dominique,|NONFICTION    |ADULT   |2022-06-07 08:46:36|\n",
      "|ANN         |Annerley    |null              |NONFICTION    |ADULT   |2022-06-07 08:46:36|\n",
      "|ANN         |Annerley    |null              |AD-MAGS       |ADULT   |2022-06-07 08:46:36|\n",
      "|ANN         |Annerley    |Damas, Jeanne,    |NONFICTION    |ADULT   |2022-06-07 08:46:36|\n",
      "|ANN         |Annerley    |Obama, Michelle,  |BIOGRAPHY     |ADULT   |2022-06-07 09:04:20|\n",
      "|ANN         |Annerley    |null              |DVD           |ADULT   |2022-06-07 09:50:07|\n",
      "|ANN         |Annerley    |null              |DVD           |ADULT   |2022-06-07 09:50:07|\n",
      "|ANN         |Annerley    |null              |DVD           |ADULT   |2022-06-07 09:50:07|\n",
      "|ANN         |Annerley    |null              |DVD           |ADULT   |2022-06-07 09:50:07|\n",
      "|ANN         |Annerley    |null              |DVD           |ADULT   |2022-06-07 09:50:07|\n",
      "|ANN         |Annerley    |null              |DVD           |ADULT   |2022-06-07 09:50:07|\n",
      "|ANN         |Annerley    |null              |DVD           |ADULT   |2022-06-07 09:50:21|\n",
      "|ANN         |Annerley    |Friedl, Reinhard, |NONFICTION    |ADULT   |2022-06-07 09:50:22|\n",
      "|ANN         |Annerley    |Nathan, Alix,     |AD-FICTION    |ADULT   |2022-06-07 10:09:31|\n",
      "|ANN         |Annerley    |Woods, Stuart,    |AD-FICTION    |ADULT   |2022-06-07 10:09:31|\n",
      "|ANN         |Annerley    |Patterson, James, |AD-PBK        |ADULT   |2022-06-07 10:09:31|\n",
      "|ANN         |Annerley    |Woods, Stuart,    |AD-PBK        |ADULT   |2022-06-07 10:09:35|\n",
      "+------------+------------+------------------+--------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df.select(\"library_code\", \"library_name\", \"title\", \"author\", \"item_type_code\", \"age\", \"date\")\n",
    "\n",
    "(df_exp.orderBy(\"library_code\", \"date\").drop(\"title\")\n",
    "    .show(truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we group sets of checkouts together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_records_into_checkouts(df, allowed_gap_in_seconds, show_intermediate=True):\n",
    "    # Add a row number to act as a tie break when ordering identical dates\n",
    "    df_checkout = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"library_code\").orderBy(\"date\")))\n",
    "\n",
    "    # Grab the time of the immediately previous checkout\n",
    "    df_checkout = df_checkout.withColumn(\"previous_checkout\", F.lag(F.col(\"date\")).over(Window.partitionBy(\"library_code\").orderBy(\"date\", \"row_number\")))\n",
    "\n",
    "    # Only consider it to be the same checkout if the previous occurred within the previous X seconds\n",
    "    df_checkout = (df_checkout.withColumn(\"time_between_checkouts\", F.unix_timestamp(\"date\") - F.unix_timestamp(\"previous_checkout\"))\n",
    "                    .withColumn(\"is_same_checkout\", F.col(\"time_between_checkouts\") <= allowed_gap_in_seconds)\n",
    "                    .withColumn(\"new_checkout_increment\", F.when(F.col(\"is_same_checkout\"), 0).otherwise(1))\n",
    "                    .drop(\"is_same_checkout\"))\n",
    "\n",
    "    # The checkout IDs can be generated by doing a cumulative sum \n",
    "    df_checkout = df_checkout.withColumn(\"checkout_id\", F.sum(\"new_checkout_increment\")\n",
    "                                            .over(Window.partitionBy(\"library_code\").orderBy(\"date\", \"row_number\").rangeBetween(Window.unboundedPreceding, 0)))\n",
    "\n",
    "    if show_intermediate:\n",
    "        df_checkout.drop(\"title\", \"author\", \"item_type_code\", \"age\").orderBy(\"library_code\", \"date\", \"row_number\").show(5)\n",
    "\n",
    "    return df_checkout.drop(\"row_number\", \"previous_checkout\", \"new_checkout_increment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Group the records into clusters with unique checkout_ids\n",
    "2. Print statistics about the checkout_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------------+----------+-------------------+----------------------+----------------------+-----------+\n",
      "|library_code|library_name|               date|row_number|  previous_checkout|time_between_checkouts|new_checkout_increment|checkout_id|\n",
      "+------------+------------+-------------------+----------+-------------------+----------------------+----------------------+-----------+\n",
      "|         ANN|    Annerley|2022-06-07 06:22:16|         1|               null|                  null|                     1|          1|\n",
      "|         ANN|    Annerley|2022-06-07 06:22:23|         2|2022-06-07 06:22:16|                     7|                     1|          2|\n",
      "|         ANN|    Annerley|2022-06-07 06:23:36|         3|2022-06-07 06:22:23|                    73|                     1|          3|\n",
      "|         ANN|    Annerley|2022-06-07 08:46:36|         4|2022-06-07 06:23:36|                  8580|                     1|          4|\n",
      "|         ANN|    Annerley|2022-06-07 08:46:36|         5|2022-06-07 08:46:36|                     0|                     0|          4|\n",
      "+------------+------------+-------------------+----------+-------------------+----------------------+----------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|            count|\n",
      "+-------+-----------------+\n",
      "|  count|            20651|\n",
      "|   mean|2.885719819863445|\n",
      "| stddev|2.839502554863082|\n",
      "|    min|                1|\n",
      "|    max|               62|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_checkout = group_records_into_checkouts(df_exp, 5)\n",
    "\n",
    "(df_checkout.groupBy(\"library_code\", \"checkout_id\")\n",
    "                       .count()\n",
    "                       .select(\"count\")\n",
    "                       .describe()\n",
    "                       .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate some checkout cluster statistics (count, duration)\n",
    "2. Get the checkout cluster with the largest number of checkouts\n",
    "3. Print the cluster stats\n",
    "3. Print all of the records in that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+-------------------+-------------------+--------+\n",
      "|library_code|checkout_id|count|           max_date|           min_date|duration|\n",
      "+------------+-----------+-----+-------------------+-------------------+--------+\n",
      "|         SBK|        293|   62|2022-06-07 15:28:15|2022-06-07 15:27:26|      49|\n",
      "+------------+-----------+-----+-------------------+-------------------+--------+\n",
      "\n",
      "+-----------+----------------------------------------------------------------------------+----------------------+--------------+--------+-------------------+----------------------+\n",
      "|checkout_id|title                                                                       |author                |item_type_code|age     |date               |time_between_checkouts|\n",
      "+-----------+----------------------------------------------------------------------------+----------------------+--------------+--------+-------------------+----------------------+\n",
      "|293        |Money magazine.                                                             |null                  |AD-MAGS       |ADULT   |2022-06-07 15:27:26|101                   |\n",
      "|293        |Money magazine.                                                             |null                  |AD-MAGS       |ADULT   |2022-06-07 15:27:27|1                     |\n",
      "|293        |A merry Shopkins Christmas /                                                |Rusu, Meredith,       |PICTURE-BK    |JUVENILE|2022-06-07 15:27:28|1                     |\n",
      "|293        |Money magazine.                                                             |null                  |AD-MAGS       |ADULT   |2022-06-07 15:27:28|0                     |\n",
      "|293        |The rubbish trucks /                                                        |Budgell, Gill,        |JU-READER     |JUVENILE|2022-06-07 15:27:29|1                     |\n",
      "|293        |The Gwibber /                                                               |Thompson, Lisa,       |JU-PBK        |JUVENILE|2022-06-07 15:27:30|1                     |\n",
      "|293        |Town under attack /                                                         |Glennie, Anne,        |JU-READER     |JUVENILE|2022-06-07 15:27:31|0                     |\n",
      "|293        |Double the dinosaurs /                                                      |Murray, Diana,        |JU-READER     |JUVENILE|2022-06-07 15:27:31|1                     |\n",
      "|293        |Little lost dinosaur /                                                      |Thompson, Lisa,       |JU-PBK        |JUVENILE|2022-06-07 15:27:32|1                     |\n",
      "|293        |Best friends /                                                              |Cuyler, Margery,      |JU-READER     |JUVENILE|2022-06-07 15:27:34|2                     |\n",
      "|293        |How to start kindergarten /                                                 |Hapka, Cathy,         |JU-READER     |JUVENILE|2022-06-07 15:27:35|0                     |\n",
      "|293        |Around the island /                                                         |Thompson, Lisa,       |JU-PBK        |JUVENILE|2022-06-07 15:27:35|1                     |\n",
      "|293        |A bad case of the hiccups /                                                 |Penney, Shannon,      |JU-READER     |JUVENILE|2022-06-07 15:27:36|1                     |\n",
      "|293        |Boku wa tane /                                                              |Ueda, Risa,           |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:37|1                     |\n",
      "|293        |Uss?su yakisoba /                                                           |Sasaki, Mio,          |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:37|0                     |\n",
      "|293        |Papatora /                                                                  |Hirata, Masahiro,     |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:38|1                     |\n",
      "|293        |Nukadokosuke /                                                              |Kat?, Mafumi,         |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:39|0                     |\n",
      "|293        |Denshagokko /                                                               |Arai, Hiroyuki,       |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:39|1                     |\n",
      "|293        |Pan densha /                                                                |Arita, Nao,           |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:40|1                     |\n",
      "|293        |Bluey.                                                                      |null                  |DVD           |JUVENILE|2022-06-07 15:27:41|1                     |\n",
      "|293        |Gy?retsu iroiro /                                                           |Akkototo,             |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:41|0                     |\n",
      "|293        |Chanoma no ozabuton /                                                       |Kaneko, Maki,         |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:42|1                     |\n",
      "|293        |Hataraku taoru /                                                            |Nishizawa, Yukari,    |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:43|1                     |\n",
      "|293        |Yonde yonde /                                                               |Tokiwa, Hiromi,       |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:43|0                     |\n",
      "|293        |Densha no tsukurikata /                                                     |Mizoguchi, Itaru,     |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:44|0                     |\n",
      "|293        |Arigat? /                                                                   |Arai, Hiroyuki,       |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:44|1                     |\n",
      "|293        |Dennya /                                                                    |?tsuka, Kenta,        |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:45|1                     |\n",
      "|293        |Mogura no ie /                                                              |Matsuya, Mayuko,      |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:46|1                     |\n",
      "|293        |Os?ji robotto no Kyukyu /                                                   |Komori, Makoto,       |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:46|0                     |\n",
      "|293        |Banana oishiku n?re /                                                       |Yano, Akemi,          |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:47|1                     |\n",
      "|293        |The capsule wardrobe : 1,000 outfits from 30 pieces /                       |Mak, Wendy,           |NONFICTION    |ADULT   |2022-06-07 15:27:47|0                     |\n",
      "|293        |In the park /                                                               |Clarke, Zo�,          |JU-READER     |JUVENILE|2022-06-07 15:27:48|0                     |\n",
      "|293        |Don't throw it to Mo! /                                                     |Adler, David A.,      |JU-READER     |JUVENILE|2022-06-07 15:27:48|1                     |\n",
      "|293        |Ky?ry? no ?kisa tte dorekurai? /                                            |?shima, Eitar?,       |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:49|1                     |\n",
      "|293        |Kore wa wani desu /                                                         |McKenzie, Heath,      |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:50|1                     |\n",
      "|293        |Ky?ry? Torikeratopusu to Ketsarukoatorusu : kyodai yokury? to tatakau maki /|Kurokawa, Mitsuhiro,  |LOTE-BOOK     |JUVENILE|2022-06-07 15:27:50|0                     |\n",
      "|293        |Choo-choo school /                                                          |Rosenthal, Amy Krouse,|PICTURE-BK    |JUVENILE|2022-06-07 15:27:51|1                     |\n",
      "|293        |Beast-like dinosaurs : theropods /                                          |Johnson, Rebecca,     |JU-READER     |JUVENILE|2022-06-07 15:27:52|1                     |\n",
      "|293        |Kangaroos                                                                   |Johnson, Rebecca,     |JU-READER     |JUVENILE|2022-06-07 15:27:52|0                     |\n",
      "|293        |A to Z of dinosaurs and prehistoric animals /                               |Dickmann, Nancy,      |NONFICTION    |JUVENILE|2022-06-07 15:27:53|1                     |\n",
      "|293        |Why do we sleep? /                                                          |Claybourne, Anna,     |JU-READER     |JUVENILE|2022-06-07 15:27:54|0                     |\n",
      "|293        |Top hunters /                                                               |Llewellyn, Claire,    |JU-READER     |JUVENILE|2022-06-07 15:27:54|1                     |\n",
      "|293        |Odd ocean! : all true and unbelievable! /                                   |Freels, Korynn,       |JU-READER     |JUVENILE|2022-06-07 15:27:55|1                     |\n",
      "|293        |Big bad biteasaurus /                                                       |Doyle, Malachy,       |JU-READER     |JUVENILE|2022-06-07 15:27:56|1                     |\n",
      "|293        |Dogs with jobs /                                                            |Freels, Korynn,       |JU-READER     |JUVENILE|2022-06-07 15:27:56|0                     |\n",
      "|293        |Mix and mash /                                                              |Skinner, Hatty,       |JU-READER     |JUVENILE|2022-06-07 15:27:57|1                     |\n",
      "|293        |Rocket! /                                                                   |McFarlane, Karra,     |JU-READER     |JUVENILE|2022-06-07 15:27:58|1                     |\n",
      "|293        |The dinosaur who roared for more /                                          |Punter, Russell,      |JU-READER     |JUVENILE|2022-06-07 15:27:58|0                     |\n",
      "|293        |Ribbit rabbit robot /                                                       |Mackinlay, Victoria,  |PICTURE-BK    |JUVENILE|2022-06-07 15:27:59|1                     |\n",
      "|293        |The dinosaur who ran the store /                                            |Punter, Russell,      |JU-READER     |JUVENILE|2022-06-07 15:28:00|1                     |\n",
      "|293        |Yellow Truck Road Train /                                                   |Tootell, Mandy,       |PICTURE-BK    |JUVENILE|2022-06-07 15:28:01|1                     |\n",
      "|293        |Engilina's trains /                                                         |King, Andrew,         |PICTURE-BK    |JUVENILE|2022-06-07 15:28:02|1                     |\n",
      "|293        |Sleep train /                                                               |London, Jonathan,     |PICTURE-BK    |JUVENILE|2022-06-07 15:28:03|1                     |\n",
      "|293        |No-Bot : the robot's new bottom! /                                          |Hendra, Sue,          |PICTURE-BK    |JUVENILE|2022-06-07 15:28:04|1                     |\n",
      "|293        |The Polar Express /                                                         |Van Allsburg, Chris,  |PICTURE-BK    |JUVENILE|2022-06-07 15:28:04|0                     |\n",
      "|293        |Train driver : an action play book /                                        |Green, Dan,           |PICTURE-BK    |JUVENILE|2022-06-07 15:28:05|1                     |\n",
      "|293        |The phantom bandit /                                                        |Stilton, Geronimo,    |JU-PBK        |JUVENILE|2022-06-07 15:28:06|1                     |\n",
      "|293        |Duggee and the Squirrels /                                                  |Archer, Mandy,        |PICTURE-BK    |JUVENILE|2022-06-07 15:28:07|1                     |\n",
      "|293        |Thomas and the dinosaurs /                                                  |Riordan, Jane,        |PICTURE-BK    |JUVENILE|2022-06-07 15:28:08|1                     |\n",
      "|293        |My dad is a robot /                                                         |Cosgrove, Matt,       |PICTURE-BK    |JUVENILE|2022-06-07 15:28:13|5                     |\n",
      "|293        |Astro the robot dog /                                                       |Freedman, Claire,     |PICTURE-BK    |JUVENILE|2022-06-07 15:28:14|1                     |\n",
      "|293        |Dinosaur! /                                                                 |Woodward, John,       |NONFICTION    |JUVENILE|2022-06-07 15:28:15|1                     |\n",
      "+-----------+----------------------------------------------------------------------------+----------------------+--------------+--------+-------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_record = (df_checkout.groupBy(\"library_code\", \"checkout_id\")\n",
    "                         .agg(F.count(\"library_code\").alias(\"count\"), \n",
    "                              F.max(\"date\").alias(\"max_date\"),\n",
    "                              F.min(\"date\").alias(\"min_date\"))\n",
    "                         .withColumn(\"duration\", F.unix_timestamp(F.col(\"max_date\")) - F.unix_timestamp(F.col(\"min_date\")))\n",
    "                         .orderBy(\"count\", ascending=False)\n",
    "                         .limit(1))\n",
    "\n",
    "top_record.show()\n",
    "\n",
    "biggest_checkout = df_checkout.join(top_record, [\"library_code\", \"checkout_id\"])\n",
    "\n",
    "(biggest_checkout.drop(\"library_code\", \"library_name\", \"count\", \"min_date\", \"max_date\", \"duration\")\n",
    "                 .orderBy(\"date\")\n",
    "                 .show(100, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter,Buffer(library_code))\n",
      ":- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25 AS library_code#130, date#121]\n",
      ":  +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, to_timestamp(date#26, Some(yyyyMMddHHmmss), TimestampType, Some(Australia/Sydney)) AS date#121]\n",
      ":     +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, date#26]\n",
      ":        +- Relation [title#17,author#18,call_number#19,item_id#20,item_type_code#21,status#22,language#23,age#24,checkout_library#25,date#26] csv\n",
      "+- Relation [library_code#117,library_name#118] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "library_code: string, title: string, author: string, call_number: string, item_id: string, item_type_code: string, age: string, date: timestamp, library_name: string\n",
      "Project [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118]\n",
      "+- Join LeftOuter, (library_code#130 = library_code#117)\n",
      "   :- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25 AS library_code#130, date#121]\n",
      "   :  +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, to_timestamp(date#26, Some(yyyyMMddHHmmss), TimestampType, Some(Australia/Sydney)) AS date#121]\n",
      "   :     +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, date#26]\n",
      "   :        +- Relation [title#17,author#18,call_number#19,item_id#20,item_type_code#21,status#22,language#23,age#24,checkout_library#25,date#26] csv\n",
      "   +- Relation [library_code#117,library_name#118] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118]\n",
      "+- Join LeftOuter, (library_code#130 = library_code#117)\n",
      "   :- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25 AS library_code#130, gettimestamp(date#26, yyyyMMddHHmmss, TimestampType, Some(Australia/Sydney), false) AS date#121]\n",
      "   :  +- Relation [title#17,author#18,call_number#19,item_id#20,item_type_code#21,status#22,language#23,age#24,checkout_library#25,date#26] csv\n",
      "   +- Filter isnotnull(library_code#117)\n",
      "      +- Relation [library_code#117,library_name#118] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118]\n",
      "   +- BroadcastHashJoin [library_code#130], [library_code#117], LeftOuter, BuildRight, false\n",
      "      :- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25 AS library_code#130, gettimestamp(date#26, yyyyMMddHHmmss, TimestampType, Some(Australia/Sydney), false) AS date#121]\n",
      "      :  +- FileScan csv [title#17,author#18,call_number#19,item_id#20,item_type_code#21,age#24,checkout_library#25,date#26] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/spyro/Coding/pyspark_demo/library_checkouts_202206.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<title:string,author:string,call_number:string,item_id:string,item_type_code:string,age:str...\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [id=#2673]\n",
      "         +- Filter isnotnull(library_code#117)\n",
      "            +- FileScan csv [library_code#117,library_name#118] Batched: false, DataFilters: [isnotnull(library_code#117)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/spyro/Coding/pyspark_demo/library_mapping.csv], PartitionFilters: [], PushedFilters: [IsNotNull(library_code)], ReadSchema: struct<library_code:string,library_name:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59593\n",
      "59593\n",
      "59593\n",
      "59593\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(df.count())\n",
    "print(df.count())\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59593\n"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'test' does not exist. Did you mean one of the following? [age, date, title, author, item_id, call_number, item_type_code, library_code, library_name];\n'Project ['test]\n+- Project [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118]\n   +- Join LeftOuter, (library_code#130 = library_code#117)\n      :- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25 AS library_code#130, date#121]\n      :  +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, to_timestamp(date#26, Some(yyyyMMddHHmmss), TimestampType, Some(Australia/Sydney)) AS date#121]\n      :     +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, date#26]\n      :        +- Relation [title#17,author#18,call_number#19,item_id#20,item_type_code#21,status#22,language#23,age#24,checkout_library#25,date#26] csv\n      +- Relation [library_code#117,library_name#118] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/spyro/Coding/pyspark_demo/demo.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/spyro/Coding/pyspark_demo/demo.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols: \u001b[39m\"\u001b[39m\u001b[39mColumnOrName\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m     \u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \n\u001b[1;32m   2005\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[39m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2023\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   2024\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'test' does not exist. Did you mean one of the following? [age, date, title, author, item_id, call_number, item_type_code, library_code, library_name];\n'Project ['test]\n+- Project [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118]\n   +- Join LeftOuter, (library_code#130 = library_code#117)\n      :- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25 AS library_code#130, date#121]\n      :  +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, to_timestamp(date#26, Some(yyyyMMddHHmmss), TimestampType, Some(Australia/Sydney)) AS date#121]\n      :     +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, date#26]\n      :        +- Relation [title#17,author#18,call_number#19,item_id#20,item_type_code#21,status#22,language#23,age#24,checkout_library#25,date#26] csv\n      +- Relation [library_code#117,library_name#118] csv\n"
     ]
    }
   ],
   "source": [
    "df.select(\"test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "grouping expressions sequence is empty, and 'library_code' is not an aggregate function. Wrap '(max(library_name) AS test)' in windowing function(s) or wrap 'library_code' in first() (or first_value) if you don't care which value you get.;\nAggregate [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118, max(library_name#118) AS test#1180]\n+- Project [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118]\n   +- Join LeftOuter, (library_code#130 = library_code#117)\n      :- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25 AS library_code#130, date#121]\n      :  +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, to_timestamp(date#26, Some(yyyyMMddHHmmss), TimestampType, Some(Australia/Sydney)) AS date#121]\n      :     +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, date#26]\n      :        +- Relation [title#17,author#18,call_number#19,item_id#20,item_type_code#21,status#22,language#23,age#24,checkout_library#25,date#26] csv\n      +- Relation [library_code#117,library_name#118] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/spyro/Coding/pyspark_demo/demo.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/spyro/Coding/pyspark_demo/demo.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mwithColumn(\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m, F\u001b[39m.\u001b[39;49mmax(F\u001b[39m.\u001b[39;49mcol(\u001b[39m\"\u001b[39;49m\u001b[39mlibrary_name\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[1;32m   3035\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcol should be Column\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 3036\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: grouping expressions sequence is empty, and 'library_code' is not an aggregate function. Wrap '(max(library_name) AS test)' in windowing function(s) or wrap 'library_code' in first() (or first_value) if you don't care which value you get.;\nAggregate [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118, max(library_name#118) AS test#1180]\n+- Project [library_code#130, title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, date#121, library_name#118]\n   +- Join LeftOuter, (library_code#130 = library_code#117)\n      :- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25 AS library_code#130, date#121]\n      :  +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, to_timestamp(date#26, Some(yyyyMMddHHmmss), TimestampType, Some(Australia/Sydney)) AS date#121]\n      :     +- Project [title#17, author#18, call_number#19, item_id#20, item_type_code#21, age#24, checkout_library#25, date#26]\n      :        +- Relation [title#17,author#18,call_number#19,item_id#20,item_type_code#21,status#22,language#23,age#24,checkout_library#25,date#26] csv\n      +- Relation [library_code#117,library_name#118] csv\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"test\", F.max(F.col(\"library_name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
